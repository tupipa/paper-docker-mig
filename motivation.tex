In this section, we further discuss the motivations by answering
questions: why edge applications need computation offloading, why we need  
service handoff in edge computing, why we need migration for service handoff, and why we want to do service handoff via container migration.

\subsection{Emerging Applications in Edge Computing Call For Computation Offloading}

With the rapid development of edge computing, many researchers have built various applications to take the advantages of the edge computing platforms. 
One example of such services is Augmented Reality (AR), where the application on a mobile device overlays augmented reality content
onto objects viewed on the device's camera and edge servers can
provide local object tracking and local AR content caching
\cite{satya2009case,MEC2014initiative,MEC2015-5G,hao2017challenges}.
% The solution minimizes round trip time and maximizes throughput for optimum quality of experience. 
% It can be used to offer consumer or enterprise propositions, such as tourist information, sporting event information, advertisements etc.
%
% Sataya Gabriel 
Gabriel platform \cite{ha2014wearable}  was proposed within the contexts of wearable cognitive assistance applications using a Glass-like wearable device, such as Lego Assistant, Drawing Assistant, Ping-pong Assistant.%,  and Assistance from Crowd-sourced Videos. 
Those applications need to react to a user's actions in real time with predefined guidance or guidance from crowd-sourced videos. 
%
%
% openface
OpenFace\cite{openface2016} is a real-time face recognition application for mobile that offers high accuracy with low training and prediction times. Mobile clients send the captured pictures from the camera to the nearby server. The server runs face recognition task with deep neural networks and send symbolic feedback to mobile clients in real time.  
More edge applications can be found in \cite{yi2015fog,yi2015survey,satya2017edge}.
%
All those edge applications need to offload intensive computations (e.g. computer vision, signal processing) to the edge server in order to get real time response. However, there are challenges before we can pragmatically deploy computation offloading in edge computing architectures. 

\subsection{Effective Edge Offloading Needs Seamless Handoff}

As we have mentioned, most edge applications can greatly benefit from offloading its heavy operators to the edge. However, the real time responsive service largely relies on the relative near distance to the edge servers under a WAN network. When the mobile client moves far away increasing the its distance to the current edge server, offloading performance bonus will be diminished dramatically.
Therefore, effective edge offloading needs mobility support.

In the centralized cloud infrastructure, mobility of clients can be well supported since the cloud server is always connected by clients through the long latency WAN internet. However, in the edge computing infrastructure, mobile devices are connected to the nearby edge server within its limited network coverage scope. Therefore, when the mobile device moves out of the edge server's coverage, it might be out of service, or alternatively, it has to ``handoff'' from the current edge server to a nearer edge server. 
This is an analogy to the \textit{seamless handoff} or \textit{handover}  mechanism in the cellular networks, where the moving client can seamlessly handoff across base stations and be served by its nearest available station. 
%
Therefore one of the primary needs for edge computing is to enable the free mobility for the offloading computations on the edge servers along with the mobile devices via seamless handoff.


\subsection{Seamless Edge Handoff via VM Migration}

However, there exists one key difference between the cellular network handoff and edge server handoff.
%
In cellular networks, changing a base station for a client is as simple as rebuilding a wireless connection. Most of the run-time states of the services are still available either on the client or on the remote server. After re-connection, the run-time state can be quickly seamlessly resumed. 
%
In the edge infrastructure, most mobile devices are using edge server to offload the resource-hungry or computation-intensive computations. This means that the edge server needs to hold the states of all those resource intensive workloads. 
In our mobile handoff from one edge server to another, just rebuilding the connection is certainly not enough. Since all the offloaded workloads are also need to be transferred from the current edge server to the next edge server.  

One possible solution is to use the live migration\cite{vmlivemig} to migrate a virtual machine from on edge server to another in order to seamlessly transfer the offloading workloads. However, this approach is already shown to be not suitable for edge computing environment in  \cite{ha2015vmhandoff}. Firstly, live migration and service handoff are optimized for very different performance metrics. While live migration aims to reduce the \textit{downtime} of the virtual machine, service handoff aims to reduce the \textit{total time} to the completion upon the handoff request is received. This is well discussed in \cite{ha2015vmhandoff}.
Secondly, live migration is originally designed for high performance data centers with high bandwidth network. However, this is not possible for edge servers which are deployed on the edge network over the WAN. Furthermore, live migration relies on shared storage so that it transfers only run-time state and doesn't transfer the disk storage data. Apparently, shared storage across the edge computing infrastructure is not feasible due to its widely distributed nature and low bandwidth across WAN. 

In order to enable handoff across edge computing servers, many more researches have been done based on virtual machine ~\cite{satya2009case,ha2015vmhandoff}. 
However, the total handoff time was still several minutes on a WAN network. For example, it will requires $245.5$ seconds to migrate a running OpenFace instance under 5Mbps bandwidth (50ms latency) network in \cite{ha2015vmhandoff}. 

One of the reasons for the long latency of handoff the large image size we need to transfer. Although the VM synthesis could reduce the image size by splitting images into multiply layers and only transfer the application-specific layer, the total transferred size is still in the magnitude of tens of megabytes and even hundreds of megabytes. 
The application layer was encapsulated with the whole application, both its static binary programs and its runtime memory data, which we think is an unnecessary cost. 

On the other hand, the deployment of the VM synthesis system is challenging for the legacy system. In order to enable VM synthesis, the hypervisor of VMs need to be patched to track the dirty memory at runtime. Also the storage of VM images need to be adapted to Linux FUSE interfaces in order to track file system changes inside the running VMs. Those two changes are hard to deploy in practice since they changes the behavior of hypervisors and file systems of the legacy platform and will also cause lots of performance overhead. 


\subsection{Why We Need Migration of Docker Container}
% Since handoff approaches base on VM migration posts significant performance pressure for seamless handoff of edge services, the container-based live migration has gained attractions in building efficient edge computing systems. % here we can cite quite a lot papers.
% As a container engine, Docker is increasingly popular in industrial cloud platform. 
Docker\footnote{Through out paper, we use Docker as our container engine implicitly.} is an composing engine for Linux containers, where applications are running in an isolated environment based on OS-level virtualization. Docker enables layered storage inside containers, which allows fast packaging and shipping of any application as a lightweight container. Each Docker image references a list of read-only layers that represent filesystem differences. Layers are stacked on top of each other to form a base for a container's root filesystem \cite{dockerlayer}. 

This layered storage allows fast migration of containers' run-time states without having to transfer the system and application base images. With the cloud storage of container images (like in DockerHub), all the container images are available anywhere across the Internet. Therefore, before migration starts, any edge server has the chance to download the system and application images as the base images stack for the container to run on. 
During the migration, we only need to transfer the run-time memory states and the thin \textit{container layer} on top of the Docker images stack. 

Apparently, the migration of the Docker containers is a better choice than the virtual machine based approaches we introduced above. The layered storage in Docker infrastructure enlightens a great opportunity for the service handoff based on the container migration. If the migration of containers can be done very fast, we can have the near seamless offloading service handoff across the adjacent edge servers.

However, at the time this paper is written, there is no tool that can leverage Docker's layered images to migrate containers efficiently. 
% In order to do this, we must fist dive into the images layers underlying containers and avoid the transfer of application images stack. That is just transfer the ``\textit{container layer}''. 