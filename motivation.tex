
% \subsection{Latency Sensitive Mobile Applications}

\subsection{Service Hand-off in Edge Computing Infrastructure}

With the advancement of Internet of Things and Edge Computing, many emerging applications require high bandwidth and low latency of network, which could easily overwhelm the ability of centralized cloud infrastructure. This drives the clouds towards dispersion. For example, in \cite{satya2017edge}, many wearable cognitive assistance applications can offload resource-intensive computing to cloudlet servers and were shown to be able to deliver highly responsive services as well as improved scalability and privacy enforcement ability.  However, there are still lots of challenges before we can really deploy this edge cloud infrastructure. 

One of the biggest challenges is to enable the free mobility for the mobile devices from the edge servers. In the centralized cloud infrastructure, mobility of clients are well guaranteed since the cloud server is always connected through the long latency WAN internet. However, in the edge computing infrastructure, mobile devices are connected to the nearby edge servers within limited network coverage scope. Therefore, when the mobile device moves far away from the edge server, it might be out of service, or alternatively, it has to handoff from the current edge server to a nearer edge server. 

This is very similar to the \textit{seamless hand-off } mechanism in the cellular networks, where the moving client can seamlessly hand-off across base stations and be served by its nearest available station. In cellular networks, changing a base station for a client is just to rebuild a wireless connection. Most of the run-time states of the services are still available either on the client or on the remote server. After re-connection, the run-time state can be quickly resumed seamlessly. 

However, there is one key difference between the cellular network hand-off and edge server hand-off. In the edge infrastructure, most mobile devices are using edge server to offload the resource intensive computing. This means the edge server will hold the states of all those resource intensive workloads. If we mobile hand-off from one edge server to another, just rebuilding the connection is certainly not enough. All the offloaded workloads are also need to be transferred from the current edge server to the next edge server.  

One possible solution might be using the live migration to migrate a virtual machine from on edge server to another in order to seamlessly transfer the offloading workloads. However, researchers in \cite{ha2015vmhandoff} have shown this is not feasible since live migration is originally designed for large data centers with high bandwidth network which is not feasible for edge servers which are deployed on the edge of WANs. Furthermore, live migration relies on shared storage so that it transfers only run-time state and doesn't transfer the disk storage data. Apparently, shared storage across the edge computing infrastructure is not feasible due to its widely distributed and low bandwidth feature. 

In order to enable hand-off across edge computing servers, many researches have been done based on virtual machine techniques. 

% \subsection{Fog/Edge Computing}
% \subsection{Virtual Machine Live Migration}
% \subsection{Cloudlet}

\subsection{Dynamic VM Synthesis and VM Hand-off} \label{motivation:vmhandoff}

% Many researches have been done in order to accelerate the service handoff across edge servers.
Techniques based on virtual machine migration have been proposed in  \cite{ha2015vmhandoff} \cite{satya2009case} to accelerate the service handoff across edge servers. Virtual Machine live migration in data centers could not be used for service handoff, which was well discussed in \cite{ha2015vmhandoff}. Firstly, Live migration of VM requires high bandwidth of connection and transfers Gigabytes of data in each migration, where service handoff for offloading happens on low bandwidth WANs. Secondly, live migration in data centers aims at short duration of the VM down time. However, service handoff across edge servers requires to optimize the time of total operations from the beginning of handoff request. 

% Therefore, Virtual Machine (VM) synthesis was proposed in \cite{satya2009case} to offload resource-intensive mobile applications to the cloudlet under low WAN delays. VM images were splitted into two layers: base image and application overlays. The mobile device only need to transfer the application-specific overlay to the target server instead of the whole VM image. 

In order to enable high speed service hand-off based on virtual machine migration techniques, VM synthesis\ cite{satya2009case}  was first proposed to enable resource-intensive mobile applications to leverage cloud computing under low WAN delays. It divided a huge VM imges into a base VM image and multiple small-sized application-specific overlay images. The server was assumed to have a base image and mobile device could just transfer its application's overlay images to the server. Then the server would run the resource-intensive application on behalf of the resource-limited mobile devices. 


% Based on VM synthesis, VM handoff across Couldlet servers was proposed to enable mobile clients seamlessly migrating its resource-intensive applications from one sever to another \cite{ha2015vmhandoff}. However, the total handoff time was $1\sim4$ minutes on a WAN network, which is still a relatively long time for latency sensitive mobile applications.

Based on the work of VM synthesis, \cite{ha2015vmhandoff} enabled VM handoff across Couldlet servers (alias of edge servers). It allowed mobile clients seamlessly transfer its offloading workloads from one edge sever to a nearer edge server. Before the migration, it assumed both server already had the base VM image. Therefore, during VM hand-off, only the VM overlay encapsulated the application and its runtime states were transferred through the network. It highly reduced the transfer size and time compared to the solely VM migration. However, the transfer size of the application still had tens or hundreds of megabytes on average. The total hand-off time was 1$\sim$4 minutes on a WAN network.



The root cause of the long latency of handoff the large image size we need to transfer. Although the VM synthesis could reduce the image size by splitting images into multiply layers and only transfer the application-specific layer, the total transferred size is still in the magnitude of tens of megabytes and even hundreds of megabytes. The application layer was encapsulated with the whole application, both its static binary programs and its runtime memory data, which we think is an unnecessary cost. 


% \subsubsection{Enlightened VM Post-Copy} In order to improve the 

% \subsection{Docker Layered Images }

% As a container engine, Docker is increasingly popular in industrial cloud platform. It serves as an composing engine for Linux containers, where applications are running in an isolated environment based on OS-level virtualization. Docker enables layered storage inside containers, which allows fast packaging and shipping of any application as a lightweight container. Each Docker image references a list of read-only layers that represent filesystem differences. Layers are stacked on top of each other to form a base for a container’s root filesystem \cite{dockerlayer}. 
% % \cite{https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/#images-and-layers}

% When a new container is started, a new, thin, writable layer, called \textit{``container layer''} is created on top of the underlying stack. All changes made to the running container are written to this thin layer. 
% % It greatly simplifies the process of setting environment for any software platform. 

% This layered storage allows fast live migration of containers' run-time states without having to transfer the system and application base images. With the cloud storage of container images (like in DockerHub), all the container images are available anywhere across the Internet. Therefore, before any live migration starts, any edge server has the chance to download the system and application images as the base images stack for the container to run on. 
% During the migration, we only need to transfer the run-time memory states and the thin container layer on top of the Docker images stack. 


% Apparently, the live migration of the Docker containers seems to be a better choice than the virtual machine based approaches we introduced above. The layered storage in Docker infrastructure enlightens a great opportunity for the service hand-off in edge computing. If the live migration of containers can be done very fast, we can have the service hand-off built ontop of it to get high speed hand-off.

\subsection{Layered Storage of Docker Containers}

As a container engine, Docker is increasingly popular in industrial cloud platform. It serves as an composing engine for Linux containers, where applications are running in an isolated environment based on OS-level virtualization. Docker enables layered storage inside containers, which allows fast packaging and shipping of any application as a lightweight container. Each Docker image references a list of read-only layers that represent filesystem differences. Layers are stacked on top of each other to form a base for a container’s root filesystem \cite{dockerlayer}. 

This layered storage allows fast live migration of containers' run-time states without having to transfer the system and application base images. With the cloud storage of container images (like in DockerHub), all the container images are available anywhere across the Internet. Therefore, before any live migration starts, any edge server has the chance to download the system and application images as the base images stack for the container to run on. 
During the migration, we only need to transfer the run-time memory states and the thin container layer on top of the Docker images stack. 


Apparently, the live migration of the Docker containers is a better choice than the virtual machine based approaches we introduced above. The layered storage in Docker infrastructure enlightens a great opportunity for the service hand-off based on the container live migration. If the live migration of containers can be done very fast, we can have the near seamless offloading service hand-off across the adjacent edge servers.




\subsection{Docker Container Live Migration in Practice }

Although there is no official live migration tools for Docker containers yet, many enthusiastic developers have composed some customized tool for some versions of Docker platform, which shows great feasibility of Docker containers' live migration. For example, Ross Boucher's fork \cite{boucherPhaul} of P.haul
% \cite{phaul}
can be used for live migration of Docker containers for Docker 1.10-dev. However, after investigation, we found it doesn't leverage the layered images of Docker containers. It simply transfers all the file system of that container from source server to the target server, where the files are actually composed by all the image layers of that container. This made the tool not feasible and will crash the layered file system on the target Docker platform. Firstly, transferring all the image layers at once made the live migration process pretty slow for applications that has big file systems. 

To verify our claim, we conducted experiments to live migrate containers through different network connections using this tool. We experiments on one simple container, busybox, and two applications for edge server offloading: OpenFace and ?.  Busybox is a stripped-down Unix tools in a single executable file. It has very tiny file system inside the container. OpenFace\cite{openface2016} is an application that will ship images from mobile device to the edge server, then execute the face recognition algorithm on server, and finally sending back a text string of the name of the person to the mobile device. The file system is huge for this container, which is approxmitely $2$ gigabytes.

Table~\ref{table_samehost} shows the live migration could be done within 10 seconds for busybox and with 30 seconds for OpenFace. The two nodes are two virtual machines on the same physical host. The network bandwidth is high with just an RTT of 0.4 milliseconds, which allows to transfer 2.17 GB data within a short time.

\input{figure/table-phaul.tex}

However, when we live migrate containers between two physical host on the same LAN network, the performance is reduced dramatically. Table~\ref{table_wireless} shows that the migration of the busybox container will take 133.11 seconds with just 152 kilobytes of transferred size. And for the heavy offloading application of OpenFace, it cost about 3200 seconds with transferred sized of more than 2 Gigabytes. After investigation into its detailed steps of live migration, we found it didn't make use of the layered infrastructure. Instead of transfering only the thin container layer modified inside the container, it transfers the whole file system including all the stacked image layers of the container, which is definitely not a proper way to migrate Docker containers.  This behavior would cause severe problems from several aspects:
% \begin{enumerate}[series = tobecont , itemjoin = \quad]
\begin{enumerate}[series = tobecont]
    \item It will corrupt the layered file system inside the container after restored  on the target server. The tools just transfer the whole file system into one directory on the destination server and mount it as root directory for the container. After restored on the target host, the container no longer maintains its layered image stacks as on the source node. 
    \item It dramatically reduces the efficiency of live migration. The transferring size of live migrating a container with its whole file system would just worse than a mature VM live migration. Because even live migration could avoid transferring the whole file system by sharing the base VM images as shown in \ref{motivation:vmhandoff}. 
\end{enumerate}

Therefore, we need a new tool for live migrating the Docker containers. In this tool, we must dive into the layered layers underlying containers and avoid the transfer of application images stack. That is just transfer the ``container layer''. 
