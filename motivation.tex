In this section, we seek to answer the following questions: Why do edge applications need offloading of computation? Why is service handoff needed in edge computing? Why is migration needed for service handoff? Why do we seek to do service handoff via container migration?

\subsection{Emerging Applications in Edge Computing Call For Computation Offloading}

With the rapid development of edge computing, many researchers have constructed applications to take advantage of the edge computing platform. 

One such example is Augmented Reality (AR). AR applications on mobile devices overlay augmented reality content
onto objects viewed with device cameras. Edge servers can
provide local object tracking, and local AR content caching
\cite{satya2009case,MEC2014initiative,MEC2015-5G,hao2017challenges}.
% The solution minimizes round trip time and maximizes throughput for optimum quality of experience. 
% It can be used to offer consumer or enterprise propositions, such as tourist information, sporting event information, advertisements etc.
%
% Sataya Gabriel 
The Gabriel platform \cite{ha2014wearable}  was proposed within the context of wearable cognitive assistance applications using a Glass-like wearable device, such as Lego Assistant, Drawing Assistant, or Ping-pong Assistant. 
%,  and Assistance from Crowd-sourced Videos. 
Those applications need to react to user actions in real time, following predefined guidance, or guidance from crowd-sourced videos. 
%
%


% openface
OpenFace\cite{openface2016} is a real-time mobile facial recognition application that offers high accuracy along with low training and prediction times.  The mobile client sends captured pictures from the camera to a nearby server. The server is running a facial recognition service that sends symbolic feedback to the mobile client in real time.  


More edge applications can be found in \cite{yi2015fog,yi2015survey,satya2017edge}.
%
All of these edge applications need to offload intensive computations (e.g., machine learning, computer vision, signal processing, etc.) to the edge server in order to achieve real time response. However, there we face several challenges before proceeding to deploy computation offloading services in the edge computing architecture. 

\subsection{Effective Edge Offloading Needs \textit{Service Handoff}}

As has been mentioned, most edge applications can gain benefits by offloading heavy computations from mobile clients to nearby edge servers. However, responsive real time services largely rely upon relatively short network distances between the mobile client and the edge server. When the mobile client moves farther away 
% increasing the its distance to the current edge server
, benefits from offloading performance will be diminished dramatically.
Therefore, effective edge offloading needs to support mobile services and users.

In the centralized cloud infrastructure, mobility of clients can be well supported since the client is supposed to connect to the centralized cloud server through the long latency WAN internet. However, in the edge computing infrastructure, mobile devices connect to nearby edge servers to benefit from high bandwidth and low latency connections. Therefore, when the mobile device moves farther away from its edge server, it might suffer from higher latency, or even become out of service. 

In order to be continuously served by a nearby edge server, the offloading computation should be moved to a new nearby edge server from the previous server. We regard this process as a \textit{service  handoff} from the current edge server to a nearer edge server. 
This is analagous to the \textit{seamless handoff} or \textit{handover}  mechanism in cellular networks, wherein the moving client connects to the nearest available base station, maintaining connectivity to the cellular network with minimal interruption. 
%
Therefore, one of the primary requirements for edge computing is to enable  \textit{service handoff} across edge servers, so that a mobile client is always served by nearby edge servers with high bandwidth and low latency.

\subsection{Seamless Service Handoff via VM Migration}

There exists one key difference between the cellular network handoff and and edge server handoff.
%
In cellular networks, changing a base station for a client is as simple as rebuilding a wireless connection. Most run-time service states are not stored on the base station but are saved on the client, or on the remote server instead. Therefore, after re-connection, the run-time state can be seamlessly resumed through the new connection. 

%
In the edge infrastructure, mobile devices use edge servers to offload resource-hungry or computation-intensive computations. This means that the edge server needs to hold the states of all resource intensive workloads. 
When offloading services handoff from one edge server to another, just rebuilding the connection is certainly not enough. Instead, we need to transfer all the runtime states of offloaded workloads from the current edge server to the nearer edge server.  

One possible solution is 
to use virtual machine (VM) live migration\cite{vmlivemig} 
to migrate a VM from one edge server to another in order to seamlessly transfer the offloading workloads. However, this approach has already been shown to be not suitable for edge computing environments in  \cite{ha2015vmhandoff}. 
First, live migration and service handoff are optimized according to different performance metrics. While live migration aims to reduce \textit{downtime} of the VM, service handoff aims to reduce the \textit{total time} from the time when handoff request is issued to the completion time of the migration. This is well discussed in \cite{ha2015vmhandoff}.
Second, live migration is originally designed for high performance data centers with high bandwidth networks. However, this is not possible for edge servers which are deployed over the WAN. Furthermore, live migration relies on network-based storage sharing so only run-time memory state is transferred and not storage data. Apparently, network-based storage sharing across the edge computing infrastructure is not feasible due to its widely distributed nature and low WAN bandwidth between edge servers. 

In order to enable handoff across edge computing servers, much research has focused on VM migration ~\cite{satya2009case,ha2015vmhandoff}. 
However, the total handoff time was still several minutes on a WAN network. For example, it was shown it requires $245.5$ seconds to migrate a running OpenFace instance under 5Mbps bandwidth (50ms latency) network in \cite{ha2015vmhandoff}. 

One of the reasons for the long latency of handoff is the large transfer size during the VM migration. Although VM synthesis could reduce the image size by splitting images into multiple layers, and only transferring the application-specific layer, the total transferred size is still in the magnitude of tens, or even hundreds of megabytes. 
The application layer is encapsulated with the whole application, including both the static binary programs and runtime memory data. We think this is an unnecessary cost. 

On the other hand, the deployment of the VM synthesis system is challenging for the legacy system. In order to enable VM synthesis, the VM hypervisor needs to be patched to track dirty memory at runtime. Also, storage of VM images must be adapted to Linux FUSE interfaces in order to track file system changes inside running VMs. Those two changes are hard to deploy in practice since they change the behavior of legacy platform hypervisors and file systems, along with adding lots of performance overhead. 

\subsection{Why We Need Migration of Docker Container}
% Since handoff approaches base on VM migration posts significant performance pressure for seamless handoff of edge services, the container-based live migration has gained attractions in building efficient edge computing systems. % here we can cite quite a lot papers.
% As a container engine, Docker is increasingly popular in industrial cloud platform. 
Docker 
% we could avoid this claim since only Docker could support the layer storage and it's impossible to use other container engine for this paper.
% \footnote{Through out paper, we use Docker as our container engine.
% } 
is a composing engine for Linux containers, an OS-level virtualization technique, isolating applications in the same Linux kernel based on namespace and cgroup management. Docker enables layered storage inside containers.  Each Docker image references a list of read-only storage layers that represent filesystem differences. Layers are stacked hierarchically and union mounted as a container's root filesystem \cite{dockerlayer}. The layered storage allows fast packaging and shipping of any application as a lightweight container based upon sharing of common layers.

These layered images have the potential for fast migration of containers by avoiding transferrance of common image layers between two migration nodes. With container images (like in DockerHub) located in cloud storage, all the container images are available through the centralized image server. Therefore, before migration starts, an edge server has the opportunity to download the system and application images as the container base image stack. 
During the migration, we only need to transfer the run-time memory states and the thin \textit{container layer} on top of the Docker image stack. 

Apparently, the migration of Docker containers allows smaller transfer sizes than the virtual machine based approaches we introduced above. The layered storage in Docker infrastructure enables an opportunity for service handoff based on container migration. By reducing the transfer size as much as possible, we can provide a nearly seamless offloading service handoff across the adjacent edge servers on a WAN network.

However, as of this writing, there is no current tool to leverage Docker's layered images to migrate containers efficiently. In this paper, we propose our work to reduce transfer size during container migration by leveraging layered storage.
% In order to do this, we must fist dive into the images layers underlying containers and avoid the transfer of application images stack. That is just transfer the ``\textit{container layer}''. 