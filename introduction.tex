
Edge computing has attracted lots of attention both from the industry and academia in recent years\cite{satya2017edge}, \cite{yi2015fog,yi2015survey},\cite{satya2009case}. By placing the computing and storage nodes in close proximity to mobile devices or sensors, it enables highly responsive and reliable cloud services for mobile computing\cite{satya2017edge}. 

Offloading the computation to the nearest edge servers could reduce network latency and improve user experiences. When a mobile device moves away from its current edge server, the network latency will increase, which will highly reduce the performance of offloading services. Ideally, when the user moves, its offloading service running on the edge server should also move to another edge server in order to get low network latency. Therefore, migrate the computation lively from one edge server to a nearer edge server is vital important for the edge computing infrastructure. 

VM Hand-off \cite{ha2015vmhandoff} has been proposed to accelerate the service hand-off across the offloading edge servers. It divided VM images into two stacked overlays based on Virtual Machine (VM) synthesis \cite{satya2009case} techniques. So that the mobile device only need to transfer the VM's top application overlay to the target server instead of the whole VM image volume. However, the total transferred size is still in the magnitude of tens of megabytes and even hundreds of megabytes. The total handoff time was $1\sim4$ minutes on a WAN network, which is still a relatively long time for latency sensitive mobile applications. Furthermore, VM image overlays is hard to maintain and not widely available due to its limited support and deployment in the real world.

However, the wide deployment of Docker platform enlightens the possibility of high speed offloading service hand-off. As a container engine, Docker is increasingly popular in industrial cloud platform. It serves as an composing engine for Linux containers, where applications are running in an isolated environment based on the OS-level virtualization technique.
Docker's application runs in an isolated container with all its dependencies installed. Docker's storage driver uses layered images inside containers, which enables fast packaging and shipping of any application as a lightweight container. 
% With Docker platform, the application was encapsulated into containers and the storage system has inherently layered images.  

Docker's layered storage inherently supports copy-on-write where application's all writable data are encapsulated into one thin layer on top of its base image layers. This is also an excellent support for the high speed migration of services in this paper. Based on the layered storage, we can only transfer the thin writable layer on top of the container's layer stacks during migration. The application's base image layers which is usually large in size can be downloaded before the migration. This is applicable because Docker infrastructure inherently uses the cloud storage of container images (like in DockerHub), where all the container images are available anywhere across the Internet. 

But Docker itself is still in active development, and it doesn't supports live migration of its containers yet. So in order to enable offloading service hand-off based on Docker platform, we must meet the following challenges.

Firstly, we need to know the details of how Docker manages its image layers and how a container is created from layerred images. Until now, we cannot see any research work that introduce the images management inside Docker. In order to provide a systematic bird-view over Docker's storage management, we investigate and summarize the layered images based on AUFS storage driver in \ref{aufsIntroduction}.

Secondly, we need to find out how can we make use of Docker's layered images to speed up the live migration of containers. According what we found inside AUFS storage, Docker will create a new SHA256 number as local  identification for each image layer downloaded from the cloud. 
This means that if two Docker host download the same image layer from the same repository, they will have different reference identification. This is originally a safety mechanism to avoid the image overlapping across Docker hosts\cite{dockerlayer}. However, when we migrate one container from one Docker host to another Docker host, we must recognize those image layers that have different local identifications but actually have the same content with the same origin. So that those image layers will not be transferred during the migration.

Thirdly, the transferred data will include both the file system as well as the checkpointed binary memory images. Although the total size is reduced dramatically by leveraging Docker's layered storage, we still need to find a way to further reduce the transfer size in order to reduce the transfer time. 

Last but not the least, we need to find a way to mitigate the user-experience latency caused by migrating a container across different edge servers. Use experienced latency could be shorter than the actual migration time by well designed strategies of migration process. Ideally, we should make it close to a seamless service hand-off where users cannot notice their offloading edge server has been changed.

In order to meet those challenges, we proposed a framework that could enable high speed offloading service hand-off across edge servers by only encapsulate and transfer the thin writable layer of the application and its runtime status based on Docker's  experimental feature of checkpoint and restore. The contributions of this paper are listed as below:

\begin{itemize}
    
    \item It analyses the status of current techniques to do live migration for Docker containers. It evaluates the performance of some state-of-the-art works that could do Docker container live migration and expose the problems in it.
    
     \item It analyses the detailed storage management of Docker containers based on AUFS storage driver. It systematically introduces the layer management and images stacking methodology inside the Docker storage and reveals the proper way to leverage those mechanisms to speed up the live migration.
    
    
    \item It proposes a framework that enable service hand-off across edge offloading servers with help of the high speed live migration of containers leveraging the layered images of Docker platform. The framework ensures low end-to-end latency of resource-intensive cloud offloading while maintaining the high mobility of clients. 
    
    \item It implemented a prototype of the system and evaluate the hand-off performance of containers and response time of offloading applications, like OpenFace, etc. 
    
\end{itemize}