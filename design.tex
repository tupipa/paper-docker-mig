

\input{figure/figure-before-after.tex}

In this section, we introduce the design of our service hand-off framework based on Docker container migration. First, we provide a simple usage scenario, give an overview of system architecture and the algorithm of service handoff. Second, in sections \ref{design:syncImage} and \ref{design:idremapping}, we discuss our methodology of storage synchronization based on Docker image layer sharing between two different Docker hosts. Finally, in sections \ref{design:memDiff}, \ref{design:compression}, and \ref{design:pipe}, we show how to further accelerate the migration speed through memory difference transfer, file compression, pipelined and paralleled processing during the migration of Docker containers.

\subsection{System Overview}

Figure~\ref{fig:before-after} shows an exemplar usage scenario of offloading service hand-off based on Docker container migration. 
In this example (OpenFace\cite{openface2016}), the mobile client achieves real-time facial recognition by offloading workloads to an edge server. 
The mobile client continuously reads images from the camera and sends them to the edge server. 
The edge server runs the facial recognition application in a container, processes the image via deep neural networks algorithm, and finally sends the recognition result  back to the client. 

All containers are running inside Virtual Machines (for example, VM \textit{A}, VM \textit{B} in Figure \ref{fig:before-after} ). This allows users to scale up the deployment more easily and control the isolation among the applications in different levels.


Before migration, each mobile client offloads computations to its nearest edge server \textbf{A}, where all the computations are processed inside a Docker container. 
In this paper, we simply call the container which computes the offloading workloads on the server side, the \textit{offloading container}.
When the mobile client moves beyond the reach of server \textbf{A} and reaches the service area of edge server \textbf{B}, its offloading computation shall be migrated from server \textbf{A} to server \textbf{B}. This is done via migration of its offloading container, where all the runtime memory states as well as associated storage data should be synchronized to the target server \textbf{B}.

\input{figure/figure-migration-stages.tex}


Figure~\ref{fig:migstages} shows the design details of this architecture as well as the migration algorithm in multiple processing stages described below:

\begin{enumerate}[label=\textbf{S\arabic*}]

\item \label{prepare} \textbf{% Pre-download Application Image.
Synchronize Base Image Layers} Upon container start, and offering of offloading service to the client. We also dynamically predict the possible target edge servers that are nearest to the client. Then we request those target servers to start synchronizing the base image layers for that container. 

\item \label{predump} \textbf{Pre-dump Container.} Before we received the migration request, we will try to dump a snapshot of the container runtime memory and send to the possible target edge servers identified in the previous stage \ref{prepare}. The container is still running during this stage. %TODO clearify the time of this event.

\item \label{request}\textbf{Migration Request Received on Source Server}. Once the migration request is initiated, the request will be sent to the source server, which we regard as the start point of the service handoff.
% \item \label{prepare} \textbf{Remapping Layer Cache IDs.} Upon receiving the hand-off request to a certain target server, prepare for file system synchronization by remapping the image layers' local identifications on the target server.

% \item \label{firstsync} Upon receiving migration request, compress the thin container layer and send to target.

\item \label{checkpoint} \textbf{Checkpoint and Stop Container.} Upon receiving the migration request, the source edge server will checkpoint and stop the container in order to send all of the latest run-time states to the target edge server.
From this point the container will stop running on the source server and wait to be restored on the target server.

\item \label{fs-sync} \textbf{Container Layer Synchronization} After checkpointing the container, the file system will not be changed. In this step, we will send the container layer contents to the target server. At the same time, all the checkpointed runtime states and configuration files, such as \textit{state.json, config.v2.json, etc.} are also transferred to the target server. 

\item \label{daemon-Reload} \textbf{Docker Daemon Reload} After we send all the runtime states and configuration files to the target node, Docker daemon on the target node still cannot recognize the container immediately.  This is because those runtime states and configuration files are created by another Docker daemon on the source node. The daemon on target node doesn't have those configurations loaded into the runtime database. We  must reload the Docker daemon in order to reload those runtime state and configuration files just received from the source node.

\item \label{img-sync} \textbf{Get, Send, and Apply Memory Difference.} After we get checkpointed images from the final dump of the container, we then compare this final dump memory to the pre-dumped memory in stage \ref{predump}. We then could get the memory differences and send only these differences to the target server. 
% \item \label{finalsync} Synchronize the file system for the last time, updating only the changed files of the container layer before last synchronization in \ref{firstsync}.

\item \label{restore} \textbf{Restore Container.} After all transfer finish, we restore the container on the target host with all runtime status retrieved. At the same time, the system will go to stage \ref{prepare} to prepare the next iteration of service handoff in the future. Now the migration has finished and the user starts to be served from the target edge server.

\item \label{clean} \textbf{Clean Up Source Node.} Finally, on the source node, we need to clean up the footprints of the offloading service. To do this, we just simply remove the container, then all its related runtime footprints will be gone. We might need to carefully choose the right time to clean up, in case the user moves back sometime in the future. If this is the case, it will be better if we keep the old footprints extant to avoid some transmission overhead. 


\end{enumerate}


% (Before migration, both the source and target edge server have the application base images downloaded.)

% \subsection{Design Assumptions}
% 
% 

\subsection{Strategy to Synchronize Storage Layers} \label{design:syncImage}

As has been mentioned, Docker's storage driver supports layered images whereby each layer represents the summary of file system differences. A running container's layered storage is composed of one writable container layer and several read only base image layers. 

The thin writable container layer stores all the files created or modified by the newly created container from the base image. As long as the container is running, this layer is subject to change. So we postpone the synchronization of the container layer until the container is stopped.

All the base image layers are read only inside containers. Once an image layer is created, it will not be changed during the whole life cycle of all the containers running on top of it. We synchronize those base image layers as early as possible.

There are two kinds of base image layers. First, in most cases, the base image layers for the container are downloaded by \textbf{docker pull} command from the centralized image registry, like Docker Hub. All those images can be shared by downloading from the same registry. Second, the container itself can also create its own image layers by saving the current container layer as one read-only image layer. 

For these two kinds of base images, we employ different synchronization strategies. More specifically, we download the common image layers between two Docker hosts from the centralized image registry and transfer only the different image layers between two Docker hosts.

By downloading the common image layer from the registry, we reduce the traffic between two edge servers. Furthermore, given that the download could start as soon as the container is created on the source server, the download time could be considered ahead of migration start and therefore amortized .

Finally, for the base image layers created locally by the container, we transfer each such image layer as the image layer is created, regardless if the migration has started or not. 

\subsection{Layer ID Remapping} \label{design:idremapping}

As discussed in section \ref{intro:idMatching} and \ref{intro:aufs:layerIDMapping}. The downloaded images from the common registry have different cache IDs exposed to each Docker host. In order to share these common images across different Docker hosts, we need to match these image layers based upon the original IDs instead of the cache IDs.

In order to do this, we first remap the cache IDs to the original IDs and compare the original IDs on two different Docker hosts. If the two image layers on two hosts share the same original IDs, we infer that they are exactly the same image layers. 

Then, for the matched original layer IDs on both Docker hosts, we remap the original IDs to the local cache IDs on the target host. Now we have the new cache IDs on the target Docker host. Then we replace the old cache IDs on the source Docker host with the new cache IDs on the target Docker host. 

By doing so, the common image layers on the migrated container will be reset with the new cache IDs that are addressable to the Docker host on the target server. When we restore the container in the future, the file system will be mounted correctly from the shared image layers on the target server.

 For the original IDs that don't match on the two hosts, we regard them as new image layers and add them to a waiting list to transfer in step \ref{fs-sync}.



% since the target Docker daemon address each layer with its locally generated cache IDs instead of the original IDS. 

% in order to make it compatible with the local addressing system of that Docker daemon. 
% After remapping all shared image layers to their local cache IDs on the target Docker, we need also to reset the image layer stack list for the migrated container we sent to the target, so that the container's file system will be mounted correctly on the target Docker host.

% The base image layer could either be downloaded from 
% the prepare for file system synchronization by remapping the image layers' local identifications on the target server.
% download the Docker image of the offloading application from Docker Hub.


\subsection{Pre-Dump \& Dirty Memory Synchronization} \label{design:memDiff}

In order to reduce the total memory image size during hand-off, we checkpoint the container and dump a snapshot of container memory in stage \ref{predump}. This could happen as soon as the container is created, or as required, we could dump the memory when the most frequently used binary programs in the application are loaded into the memory. This snapshot of memory will be served as the base memory image for the next migration.

After the base memory image is dumped, it is transferred immediately to the server. We assume that the transfer will be finished before the hand-off starts, which is reasonable since we can send the base memory image as soon as the container starts. 
% Also, the application's Docker images on the near edge servers are also started to download after the container starts and before the hand-off starts, 
After the container starts, and before the hand-off starts, the download of application Docker images are also started on the near edge servers. 
Hence, we process those two steps in parallel to reduce the total time span. This is further discussed in section \ref{design:pipe}. Thus, upon hand-off start, we have the base memory image of the container already on the target server. 

Once the migration request is received, we checkpoint and stop the container. After getting the checkpointed memory, we do a diff operation on it from the base memory image we dumped in stage \ref{predump}. 
In this way, we only need to transfer the difference of the dump memory to the target node. 

\subsection{Data Transfer}\label{design:compression}

% During migration, the files need to be transferred includes both text files as well as binary files. 
During container migration, we mainly have 4 types of data needing to be transferred:
the layer stacks information, the file system of thin writable \textit{container layer}, the meta data files of the container, and the snapshot of container memory and image difference. Some of the data is in the form of string messages, such as layer stack information. Some data are in plain text files, such as most contents and configuration files. Memory snapshots and image differences are all binary image files. According to the feature of the files, we design different strategies to transfer the data.

The layer stacks information is sent via UNIX RPC API implementation in \cite{phaul}. This is based on a socket connection created by python scripts. This information consists of a list of SHA256 ID strings, so its quite efficiently sent as a socket message. It does not merit applying compression because the overhead of compression outweighs the benefits for those short strings.

As for other data, including the container writable layer, meta data files, the dump memory images, and image differences, we use bzip2 to compress and send via authorized ssh connection.

% \ref{predump}, \ref{img-sync}.

% For the root file system and meta data files. We have two stages of files synchronization before and after the container was checkpoints. The first synchronization in \ref{firstsync} will transfer the base file system for the container layer, so it's better to compress it. We use 'tar' command to compress and sent via SSH. For the final synchronization of the file system, since we already have most of the files transferred, we choose to use 'rsync' to send over only the changed files.

% For the checkpointed memory images, we compress them via 'tar' compression and send via SSH connections.

% Syncing smaller files individually through rsync or scp results in each file starting at least one own data packet over the net. If the file is small and the packets are many, this results in increased protocol overhead. Now count in that there are more than one data packets for each file by means of rsync protocol as well (transferring checksums, comparing...), the protocol overhead quickly builds up

% \url{http://unix.stackexchange.com/questions/30953/tar-rsync-untar-any-speed-benefit-over-just-rsync}

% TODO: more details about compression and synchronization: more memory images compression techniques (to replace tar cmd). 

% TODO: rsync vs. tar comparison


\subsection{Parallel \&  Pipelined Processing}\label{design:pipe}

With the help of parallel and pipelined processing,  we could further improve the efficiency of the whole process, and reduce the total migration time.

% From stage \ref{prepare} to stage \ref{restore}, there are lots of tasks that could be run in parallel between the source and target servers.

First, starting a container will trigger two events to run in parallel: a) downloading  images from registry in the cloud, and b) pre-dumping/sending base memory images to the potential target edge servers near the source node. Those two processes could be run at the same time in order to reduce the total time of stage \ref{prepare} and \ref{predump}. 
%When the target servers are downloading images from Registry,  source server will checkpoint  and send the running container's base memory images to the possible target server.

Second, daemon reload in stage \ref{daemon-Reload} is required on the target host, it could be triggered immediately after \ref{fs-sync}. It could be paralleled with stage \ref{img-sync}, when the source server is sending the memory difference to the target host. Stage \ref{fs-sync} cannot be paralleled with \ref{daemon-Reload}, because daemon reload on the target host requires the configuration data files sent in stage \ref{fs-sync}.

Third,
in stage \ref{fs-sync}, we use compression to send all files in the \textit{container layer} over an authorized ssh connection between the source and target host. The compression and transfer of the \textit{container layer} can be pipelined using Linux pipes.

Lastly, in stage \ref{img-sync}, after we get the final memory snapshot, we will need to identify memory differences by comparing the base memory images with the images in the new snapshot, then we send the differences to the target and patch the differences to the base memory image on the target host. This whole process could also be piplined using Linux pipes. 



\subsection{Security Isolation}

Finally, it's critical to minimize security risks to the offloading services running on the edge servers. Isolation between different services could provide a certain level of security. Our framework provides an isolated running environment for the offloading service via two layers of the system virtualization hierarchy. Different services can be isolated by running inside different \textit{Linux Containers}, and different containers are allowed to be further isolated by running in different  \textit{Virtual Machines}. 

More thorough security solutions need to be designed before this framework can be deployed to the real world. These solutions include efficient run-time monitoring, secure system updating, etc.. But here we focus on the mobility of services from the performance side, and leave most security work to the future.



