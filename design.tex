
\input{figure/figure-before-after.tex}

In this section, we introduce the design of our service hand-off framework based on live migration of Docker containers. We will first briefly introduce a simple usage scenario and give an overview of system architecture and the algorithm of service handoff. Then in section \ref{design:mapImage}, we will discuss our methodology of storage synchronization based on Docker image layers' sharing between two different Docker hosts. Finally, in section \ref{design:memDiff}, \ref{design:compression}, and \ref{design:pipe}, we will discuss more strategies to further improve the migration speed by memory difference transfer, file compression, pipelined and paralleled processing during the migration of Docker containers.

\subsection{System Overview}

Figure~\ref{fig:before-after} shows an example usage scenario of offloading service hand-off based on Docker container migration. 
For example, the mobile client runs a real time face recognition application OpenFace\cite{openface2016} where it continuously sending the edge server images from the device's camera and the offloading containers processing the image via deep neural networks algorithm and send the name of the user back to the client as feedback.

Before migration, mobile client offloads its computations to its nearest edge server \textbf{A}, where all the computations are processed inside a Docker container. 
When the mobile client moves far away from server \textbf{A} and reaches the service area of edge server \textbf{B}, it's offloading computation will be migrated from server \textbf{A} to server \textbf{B}. This is done via migration of its offloading Docker containers, where all the runtime memory states as well as its storage data are synchronized to the target server \textbf{B}.

\input{figure/figure-migration-stages.tex}


Figure~\ref{fig:migstages} shows the design details of architecture as well as the migration algorithm from different processing stages described below:

\begin{enumerate}[label=\textbf{S\arabic*}]

\item 
% \label{prepare} 
\label{prepare} 
\textbf{
% Pre-download Application Image.
Synchronize Base Image Layers
} Upon the container starts and offers offloading service to the client, we also dynamically predict the possible target edge servers that are near to the client. Then we request those target servers to synchronize the base image layers for that container. 

\item \label{predump} \textbf{Pre-dump Container.} Before we received the migration request, we will try to dump a snapshot of the container's runtime memory and send to the possible target edge servers we found in stage \ref{prepare}. The container is still running during this stage. 

\item \label{request}\textbf{Migration Request Received on Source Server}. Once the migration request is initiated, the request will be sent to the source server, and we regard as the start point of the service handoff.
% \item \label{prepare} \textbf{Remapping Layer Cache IDs.} Upon receiving the hand-off request to a certain target server, prepare for file system synchronization by remapping the image layers' local identifications on the target server.

% \item \label{firstsync} Upon receiving migration request, compress the thin container layer and send to target.

\item \label{checkpoint} \textbf{Checkpoint and Stop Container.} Upon receiving the migration request, the source edge server will checkpoint and stop the container in order to send all the latest run-time states to the target edge server.
From this point the container will stop running on the source server and waiting to be restored on the target server.

\item \label{fs-sync} \textbf{Contaienr Layer Synchronization} After checkpointed the container, the file system will not be changed. In this step, we will send the container layer's contents to the target server. At the same time, all the checkpointed runtime states and configuration files, such as \textit{state.json, config.v2.json, etc.} are also transferred to the target server. 

\item \label{daemon-Reload} \textbf{Docker Daemon Reload} After we send all the runtime state and configuration files to the target node, Docker daemon on the target node still cannot recognize the container immediately.  This is because those runtime state and configuration files are created by another Docker daemon on source node, the daemon on target node doesn't have those configurations loaded into its runtime database. So we need to reload the Docker daemon in order to reload those runtime state and configuration files it just received from the source node.

\item \label{img-sync} \textbf{Get, Send, and Apply Memory Difference.} After we get checkpointed images from the final dump of the container, we then compare this final dump memory to the pre-dumped memory in stage \ref{predump}. We then could get the memory difference and send only these difference to the target server. 
% \item \label{finalsync} Synchronize the file system for the last time, updating only the changed files of the container layer before last synchronization in \ref{firstsync}.

\item \label{restore} \textbf{Restore Container.} Finally, we will restore container on the target host with all its runtime status retrieved. At the same time, the system will go to stage \ref{prepare} to prepare the next iteration of service handoff in the future.

\end{enumerate}


% (Before migration, both the source and target edge server have the application base images downloaded.)

% \subsection{Design Assumptions}
% 
% 

\subsection{Strategy to Synchronize Storage Layers} \label{design:mapImage}

As we introduced above, Docker's storage driver supports layered images where each layer represents file system differences. A running container's layered storage is composed by one writable container layer and several read only base image layers. 

The thin writable container layer stores all the files created or modified by the new created container from the base image. As long as the container is running, this layer is subject to change. So we postpone the synchronization of container layer after the container is stopped.

All the base image layers are read only inside container. They will not be changed during the whole life cycle of the container. So we could synchronize those layers as early as possible.

There are two kinds of base image layers. Firstly, in most cases, the base image layers for the container are downloaded by `docker pull` command from the centralized Registry like Docker Hub. All those images can be shared by downloading from the same Registry. Secondly, the container itself can also create its own image layers by saving its current container layer as one read only image layer. 

For these two kinds of base images, we use different strategies to synchronize. Generally, we will download the common image layers between two Docker hosts from the centralized image Registry and transfer only the different image layers between two Docker hosts.

By downloading the common image layer from the Registry, we could reduce the traffic between two edge servers. Furthermore, the download could start as soon as the container is created on the source server, and thus the download time could be masked ahead of migration starts.

Finally, for the base image layers create by the container, we transfer each image layer upon the image layer is created, no matter whether the migration has started or not. 

\subsection{Layer ID Remapping} \label{idremapping}

As discussed in section \ref{intro:idMatching} and \ref{intro:aufs:layerIDMapping}. The downloaded images from the common Registry have different cache IDs exposed to each Docker host. In order to share these common images across different Docker host, we need to match these  image layers by its original IDs instead of the cache IDs.

In order to do this, we first remapping the cached IDs to its original IDs and compare the original IDs on two Different Docker hosts. If the two image layers on two host share the same original ID, we could determine they are exactly the same image layer. 

Then, for the matched original layer IDs on both of the Docker hosts, we will to remap the original IDs to its local cache IDs on the target host. Now that we get the new cache IDs on the target Docker host. Then we replace the old cache IDs on the source host with the new cache IDs on the target Docker host.  

By doing so, the image layer stack list for the migrated container will be reset with the new cache IDs that is addressable for the Docker host on the target server. So that when we restore the container in the future, its file system will be mounted correctly from the shared image layers on the target server.


% since the target Docker daemon address each layer with its locally generated cache IDs instead of the original IDS. 

% in order to make it compatible with the local addressing system of that Docker daemon. 
% After remapping all shared image layers to their local cache IDs on the target Docker, we need also to reset the image layer stack list for the migrated container we sent to the target, so that the container's file system will be mounted correctly on the target Docker host.

% The base image layer could either be downloaded from 
% the prepare for file system synchronization by remapping the image layers' local identifications on the target server.
% download the Docker image of the offloading application from Docker Hub.


\subsection{Pre-Dump \& Dirty Memory Synchronization} \label{design:memDiff}

In order to reduce the total memory image size during hand-off, we checkpoint the container and dump a snapshot of the container's memory in stage \ref{predump}. This could happen as soon as the container is created, or more enlightened, we could dump the memory when the most frequently used binary programs in the application are loaded into the memory. This snapshot of memory will be served as the base memory image for the live migration that happens next.

After the base memory image is dumped, it is transferred immediately to the server. We assume the transfer will be finished before the hand-off starts. It's reasonable since we can send the base memory image as early as seconds after the container starts. 
Also, the application's Docker images on the near edge servers are also started to download after the container starts and before the hand-off starts, we could process those two steps in parallel to reduce the total time cost. This is further discussed in section \ref{design:pipe}. Therefore, upon the hand-off starts, we can have the base memory image of the container already on the target server. 

Then after the client being served at the source node and starts to move into another edge server's service area, a migration request will be sent to the source node. 
% As for who send the migration request, it's out of the scope of this paper.

Once the migration request is received, we will  checkpoint and stop the container. After get the checkpointed memory, we do a diff operation on it from the base memory image we dumped in stage \ref{predump}. 
Then we only need to transfer the difference of the dump memory to the target node. 

\input{figure/dumps/diff_size.tex}

\subsection{Data Transfer}\label{design:compression}

% During migration, the files need to be transferred includes both text files as well as binary files. 
During the live migration of a container, we mainly have 4 kinds of data need to transfer:
the layer stacks information, the file system of thin writable \textit{container layer}, the meta data files of the container, and snapshot of the container's memory and image difference. Some of the data is in the form of messages, such as layer stacks information. Some of them are plain text files, such as most contents. And the memory snapshots and image differences are all binary image files. According to the feature of the files, we use different strategies to transfer the data.

The layer stacks information is send via UNIX RPC API implementation in \cite{phaul}, which is based on a socket connection created by python scripts. Those information is only a list of SHA256 ID strings, so it's quite efficient to be sent as a socket message. There is no need to compress it since the overhead of compression would mask its benefits.

As for other data, including the container writable layer, meta data files, the dump memory images, and image differences, we use bzip2 to compress and send via authorized ssh connection.

% \ref{predump}, \ref{img-sync}.

% For the root file system and meta data files. We have two stages of files synchronization before and after the container was checkpoints. The first synchronization in \ref{firstsync} will transfer the base file system for the container layer, so it's better to compress it. We use 'tar' command to compress and sent via SSH. For the final synchronization of the file system, since we already have most of the files transferred, we choose to use 'rsync' to send over only the changed files.

% For the checkpointed memory images, we compress them via 'tar' compression and send via SSH connections.

% Syncing smaller files individually through rsync or scp results in each file starting at least one own data packet over the net. If the file is small and the packets are many, this results in increased protocol overhead. Now count in that there are more than one data packets for each file by means of rsync protocol as well (transferring checksums, comparing...), the protocol overhead quickly builds up

% \url{http://unix.stackexchange.com/questions/30953/tar-rsync-untar-any-speed-benefit-over-just-rsync}

% TODO: more details about compression and synchronization: more memory images compression techniques (to replace tar cmd). 

% TODO: rsync vs. tar comparison


\subsection{Parallel \&  Pipelined Processing}\label{design:pipe}

With the help of parallel and pipelined processing,  we could improve the efficiency of the whole process and reduce the total migration time.

% From stage \ref{prepare} to stage \ref{restore}, there are lots of tasks that could be run in parallel between the source and target servers.

Firstly, as we mentioned above in \ref{design:memDiff}, starting a container will trigger two events to happen in parallel: downloading/synchroizing images from Registry in cloud, and pre-dumping/sending base memory images to the possible edge servers near the source node. Those two process could be run at the same time in order to reduce the total time of stage \ref{prepare} and \ref{predump}. When the possible target servers are downloading images from Registry, the source server will checkpointing and sending the running container's base memory images to the possible target server.

Secondly, daemon reload in stage \ref{daemon-Reload} is happend on target host, it could be processed while the source host is sending the memory difference to the target host in stage \ref{img-sync}. Stage \ref{fs-sync} cannot be paralleled with \ref{daemon-Reload}, because daemon reload on the target host requires the configuration data files sent in stage \ref{fs-sync}.

Thirdly,
in stage \ref{fs-sync}, we use compression to send all files in the \textit{container layer} over an authorized ssh connection between the source and target host. The compression and transfer of the \textit{container layer} can be pipelined using Linux pipes.

Lastly, in stage \ref{img-sync}, after we get the final memory snapshot, we will need to get memory difference by comparing the base memory images with the images in the new snapshot, then we send the difference to target and patch the difference to the base memory image on the target host. This whole process could also be piplined using Linux pipes. 
