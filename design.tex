
In this section, we introduce our design of the  service hand-off framework based on live migration of the containers. 

\subsection{System Overview}

\input{figure/figure-before-after.tex}


Figure~\ref{fig:before-after} shows the status of offloading services before and after migration. Before migration, mobile client offloads its computations to its nearest edge server \textbf{A}, where all the computations are processed inside a Docker container. When the mobile client moves far away from server \textbf{A} and reaches the service area of edge server \textbf{B}, it's offloading computation will be migrated from server \textbf{A} to server \textbf{B}. This is done via migration of its offloading Docker containers, where all the runtime memory states as well as its storage layer are transferred to server \textbf{B}.


\input{figure/figure-migration-stages.tex}


As shown in Figure~\ref{fig:migstages},
% shows the overview of the whole migration stages. Generally, 
our architecture supports high speed offloading service hand-off through the following steps:
% \begin{enumerate*}[series = tobecont, itemjoin = \quad]

\begin{enumerate}[label=\textbf{S\arabic*}]

\item \label{predownload}\textbf{Pre-download Application Image.} In this stage, we will dynamically predict the possible target edge servers and pre-download the offloading application's Docker images on each target servers.

\item \label{predump} \textbf{Pre-dump Container.} Before we received the migration request, we will try to dump a snapshot of the container's runtime memory and send to the possible target edge servers we found in stage \ref{predownload}. The container is still running during this stage. 

\item \label{prepare} \textbf{Remapping Layer Cache IDs.} Upon receiving the hand-off request to a certain target server, prepare for file system synchronization by remapping the image layers' local identifications on the target server.

% \item \label{firstsync} Upon receiving migration request, compress the thin container layer and send to target.

\item \label{checkpoint} \textbf{Checkpoint and Stop Container.} After we remapped the layer IDs according the shared original layer IDs between source and target edge servers, we then could checkpoint and stop the container in order to send all the latest run-time states to the target edge server.
From this point the container will stop running on the source server and waiting to be restored on the target server.

\item \label{fs-sync} \textbf{Contaienr Layer Synchronization} After checkpointed the container, the file system will not be changed. In this step, we will send the container layer's contents to the target server. At the same time, all the checkpointed runtime states and configuration files, such as \textit{state.json, config.v2.json, etc.} are also transferred to the target server. 

\item \label{daemon-Reload} \textbf{Docker Daemon Reload} After we send all the runtime state and configuration files to the target node, Docker daemon on the target node still cannot recognize the container immediately.  This is because those runtime state and configuration files are created by another Docker daemon on source node, the daemon on target node doesn't have those configurations loaded into its runtime database. So we need to reload the Docker daemon in order to reload those runtime state and configuration files it just received from the source node.

\item \label{img-sync} \textbf{Get Memory Difference.} After we get checkpointed images from the final dump of the container, we then compare this final dump memory to the pre-dumped memory in stage \ref{predump}. We then could get the memory difference and send only these difference to the target server. 
% \item \label{finalsync} Synchronize the file system for the last time, updating only the changed files of the container layer before last synchronization in \ref{firstsync}.

\item \label{restore} \textbf{Restore Container.} Restore container on the target host.

\end{enumerate}


% (Before migration, both the source and target edge server have the application base images downloaded.)
 
\subsection{Match Shared Image Layers}

As we introduced above, in the AUFS-based addressable layers, the local cached ID for each downloaded image layer is stored in 

\subsection{Get Memory Difference}
In order to reduce the total memory image size during hand-off, we checkpoint the container and get a snapshot of the container's memory in stage \ref{predump}. The snapshot was transferred before the hand-off starts, just as the application's Docker images, which are also downloaded before the hand-off starts.

Upon the hand-off starts, we checkpoint the container and stop it. Then we do a diff operation on the new memory dump images along with the old snapshot we got from stage \ref{predump}. Then we only need to transfer the difference of the dump memory to the target node. We use Xdelta3 to get the memory difference.

\input{figure/diff_size.tex}

\subsection{Compression of the Transferred Data}

During the live migration of a container, we mainly have 4 kinds of data need to transfer:

The layer stacks information, the container writable layer, the meta data files of the container, and the main memory images checkpointed by CRIU. 

% In order to speed up the synchronization process, we combine both the compression techniques and synchronization tools. 

The layer stacks information is send via RPC socket connection. Those information is only a list of SHA256 ID strings, so it's quite efficient to be sent as a socket message. There is no need to compress it.

The container writable layer and meta data files are regular files in the storage system, so we use bzip2  to compress and send via ssh connection.

The dump memory images are binary data, which is not efficient to compressed using bzip2. So we use ??? to compress and send via ssh connections.

\subsection{Pipelined \& Parallel Processing}


% \ref{predump}, \ref{img-sync}.

% For the root file system and meta data files. We have two stages of files synchronization before and after the container was checkpoints. The first synchronization in \ref{firstsync} will transfer the base file system for the container layer, so it's better to compress it. We use 'tar' command to compress and sent via SSH. For the final synchronization of the file system, since we already have most of the files transferred, we choose to use 'rsync' to send over only the changed files.

% For the checkpointed memory images, we compress them via 'tar' compression and send via SSH connections.

% Syncing smaller files individually through rsync or scp results in each file starting at least one own data packet over the net. If the file is small and the packets are many, this results in increased protocol overhead. Now count in that there are more than one data packets for each file by means of rsync protocol as well (transferring checksums, comparing...), the protocol overhead quickly builds up

% \url{http://unix.stackexchange.com/questions/30953/tar-rsync-untar-any-speed-benefit-over-just-rsync}

% TODO: more details about compression and synchronization: more memory images compression techniques (to replace tar cmd). 

% TODO: rsync vs. tar comparison

