
In this section, we evaluate the performance of the system from different workloads under different network conditions. 
Specifically, we evaluate the offloading our service handoff system by showing in what extent the total hand-off time and total transferred size are reduced.

\subsection{Set-Up and Benchmark Workloads}

The desktop server is equipped with Intel Core i3-6100 Processor (3.70GHz, 2 cores, 4 threads) and 16GB DDR4 memory. Two virtual machines are running with 2 vcpus and 4GB memory each.
% The laptop is with Intel$^{R}$ Core$^{TM}$ Duo T6570 (2.2GHz, 2 cores) and 2GB DDR2 memory. Both the machines and virtual machines are running Ubuntu 16.04 LTS as the operating system.
The laptop is with Intel Core i5-2450M CPU (2.5GHz, 2 cores, 4 threads) and 4GB DDR3 memory. Both the machines and virtual machines are running Ubuntu 16.04 LTS as the operating system. Docker version is 1.10-dev which is built with experimental feature enabled.

In our experiment, we set up migration scenarios using 
emulated bandwidth on two virtual machines each runs a Docker instance.
Docker containers are migrated from the Docker host on source VM to the Docker host on another VM. 
Two virtual machines run on the same physical server. 

We use Linux Traffic Control (tc\cite{tc} )  tool to control network traffic. In order to test our system under WANs , we set up the emulated low bandwidth ranged from 5Mbps to 45Mbps. According to the average bandwidth observed on the Internet\cite{internet2014state}, we set  latency a fixed 50ms to emulate the WAN network environment for edge computing.

Since edge computing environment can be also adapted to a relatively large LAN network such as a university's LAN network, or LANs for other large communities, it usually can reach higher bandwidth as well as lower latency. Therefore, we also tested serveral higher bandwidths, ranging from 50Mpbs to 1000Mpbs. The latency is set to 6ms which is an average latency from the author's university LAN network.

For the offloading workloads, we use busybox as a simple workload to show the functionality of the system as well as the non-avoidable system overhead when processing the container's live migration.  

In order to show the realtime performance of offloading service handoff, we choose OpenFace as a sample workloads.

% \subsubsection{Busybox}



% \subsubsection{OpenFace
% % : A Realtime Wearable Cognitive Application
% }

\subsection{Experimental Findings on Dirty Memory}

\input{figure/dumps-busybox/diff_size.tex}

\input{figure/dumps/diff_size.tex}

Figure \ref{fig:facediff}
and  \ref{fig:busydiff} give an overview of the dump memory image sizes while running the container of OpenFace and Busybox. 
The data is collected from 11 dumps of the running container of Busybox and OpenFace, labeled from dump 0 to 10. Memory is dumped every 10 seconds. Container continues to run during the first 10 dumps, and stops after 11th dump. From the figure we can see the memory difference is much smaller than the original dumped memory. So the migration could be speed up by only  transferring the memory difference. Furthermore, Figure \ref{fig:facediff-ori} - \ref{fig:facediffdiff-ori} show us size of memory difference between adjacent dumps as well as each dump with the original dump. We can see in the figure, iterative transferring the memory difference to the target server would be waste of efforts since the memory is continuously changing by a certain amount. Although this will not happen on every different applications, it is valuable to be considered as a special case for further researches which will live migrate the containers by iterative transferring. One way to utilize those kinds of memory access pattern is just not iterative transfer every memory difference, instead, just transfer the last iterative difference would result in the same.



\subsection{Performance Optimization }

% \subsubsection{Pipelined Processing}

\input{figure/evaluations/figure-pipes.tex}

\ref{fig:facepipe}

\ref{fig:busypipe}


\subsection{Overall Performance}
We evaluate the total time and transferred size during offloading service handoff under different bandwidths and network latency's. 

\input{figure/eva-individual/face-busy-all-bands.tex}


\input{figure/evaluations/table-optimized.tex}
Table \ref{tab:all}

\input{figure/evaluations/figure-latency.tex}
Figure \ref{fig:busylatency}

Figure \ref{fig:facelatency}

Figure \ref{fig:allband}.
Figure \ref{fig:allband-busy} 
Figure \ref{fig:allband-face}

From Table \ref{tab:all} and Figure \ref{fig:busylatency} , we can see the simple Busybox container can be migrated very quickly regardless the network bandwidth and network latency.

% \subsubsection{Performance Under Different Bandwidths}

% \subsubsection{Performance Under Different Latencies}







    % \item Laptop-Wireless-WAN:
    % The migration source host is a virtual machine on the Laptop, another is the virtual machine on the server as in 1). Both virtual machines are running the same version of Docker.  They are connected through WAN network with about 2 miles geological distance. Each virtual machines is running within its own LAN network. The virtual machine on Laptop connects to the internet through wireless adapter. The virtual machine on the server is connected to the Internet via Ethernet. 
% \end{enumerate}
