In this section, we discuss relate work in the areas of dynamic VM synthesis, VM hand-off, and container live migration.

\subsection{Dynamic VM Synthesis and VM Hand-off} \label{motivation:vmhandoff}

% Many researches have been done in order to accelerate the service handoff across edge servers.
Techniques based on virtual machine migration have been proposed in  \cite{ha2015vmhandoff} \cite{satya2009case} to accelerate the service handoff across edge servers. Virtual Machine live migration in data centers could not be used for service handoff, which was well discussed in \cite{ha2015vmhandoff}. Firstly, Live migration of VM requires high bandwidth of connection and transfers Gigabytes of data in each migration, where service handoff for offloading happens on low bandwidth WANs. Secondly, live migration in data centers aims at short duration of the VM down time. However, service handoff across edge servers requires to optimize the time of total operations from the beginning of handoff request. 

In order to enable high speed service hand-off based on virtual machine migration techniques, Satyanarayanan \textit{et al.} in \cite{satya2009case} proposed VM synthesis to divide a huge VM imges into a base VM image and a relatively small overlay image for one specific application. Before migration starts, the server is assumed to have a base image, so that the transition of this base image is avoided. During migration, only the application's overlay image is transferred from the source to the target server.

Based on the work of VM synthesis, VM handoff across Cloudlet servers (alias of edge servers) was proposed in \cite{ha2015vmhandoff}. VM handoff allows the mobile client to seamlessly transfer the runtime status of its VM from one edge sever to a nearer edge server. Besides VM synthesis, it uses dirty tracking for both file system and main memory  to reduce the transferred size during handoff.
Additionally, it also uses compression, pipelined processing to reduce transfer time and algorithms to adapt to different network conditions.
Finally, it highly reduced the transferred size and migration time under WAN network environments.

% However,
% % the transfer size of the application still had tens or hundreds of megabytes on average. 
% the total hand-off time was still several minutes on a WAN network. For example, it will requires $245.5$ seconds to migrate a running OpenFace instance under 5Mbps bandwidth (50ms latency) network in \cite{ha2015vmhandoff}. 

% One of the reasons for the long latency of handoff the large image size we need to transfer. Although the VM synthesis could reduce the image size by splitting images into multiply layers and only transfer the application-specific layer, the total transferred size is still in the magnitude of tens of megabytes and even hundreds of megabytes. 
% The application layer was encapsulated with the whole application, both its static binary programs and its runtime memory data, which we think is an unnecessary cost. 

% On the other hand, the deployment of the VM synthesis system is challenging for the legacy system. In order to enable VM synthesis, the hypervisor of VMs need to be patched to track the dirty memory at runtime. Also the storage of VM images need to be adapted to Linux FUSE interfaces in order to track file system changes inside the running VMs. Those two changes are hard to deploy in practice since they changes the behavior of hypervisors and file systems of the legacy platform and will also cause lots of performance overhead. 

% \subsection{Latency Sensitive Mobile Applications}

% \subsubsection{Enlightened VM Post-Copy} In order to improve the 
% \subsection{Linux Containers}

\subsection{Containers and Live Migration}
% Useful link: https://coreos.com/rkt/docs/latest/rkt-vs-other-projects.html#rkt-vs-runc

% Containers provide operating system level virtualization by running processes in isolated environments, 
Containers provide operating system level virtualization by running a group of processes in isolated environments. It's supported by the kernel features of namespace 
 and cgroup (control groups)\cite{rosen2014container}.
Namespaces are used to provide a private view of system resources for processes \cite{pike1992plan9}, while cgroups are used to restrict how many resources a group of processes can be accessed\cite{rosen2013namespaceIntro}.  

%  every resource in the system, either local or remote, is represented by a hierarchical file system; and a user or process assembles a private view of the system by constructing a file name space that connects these resources.

Container runtime is a tool that provides easy-to-use API for managing containers by abstracting the low-level technical details of namespaces and cgroups. 
Although creating a container by crafting cgroup and namepaces step by step is possible \cite{conScratch}, managing containers by container runtime is much more easier.  Such tools include LXC/LXD\cite{LXC} ,
runC\cite{runc},
rkt\cite{rkt}, 
OpenVZ\cite{openvz}, 
Docker\cite{docker}, etc. Different container runtime has different scenerios of usage. For example, LXC/LXD only cares about full system containers and doesnâ€™t care about what kind of application runs inside the container, while Docker aims to encapsulate a specific application in the container.  

Live migration of containers become possible when CRIU (Checkpoint/Restore In Userspace)\cite{criu} supports the checkpoint/restore functionality for Linux. 
Now CRIU supports the checkpoint and restore of containers for OpenVZ, LXC, and Docker. 


Based on CRIU, OpenVZ now supports live migration of containers. The implementation can be found from the CRIU community's open source project, P.Haul(process Hauler)\cite{phaul}.  It's claimed that the migration could be done within 5 seconds\cite{livmigcon}.
However, OpenVZ uses distributed storage system\cite{openvzfs}, where all files are shared across high bandwidth network. 
This means when it live migrates the containers, it only needs to migrate the checkpointed memory images and no file transfer need to be done. 
However, due to the limited WAN bandwidth for edge servers, it's not possible to deploy distributed storage. Therefore, live migration of OpenVZ containers is not suitable for service handoff on edge computing platforms.

For LXC containers, the live migration is implemented in Qiu's thesis work \cite{qiu2016evaluating}, which is also based on the CRIU project. However, LXC regards container as a whole system container, and they don't have layered storage for containers. Therefore, when containers are migrated, all the file system for that container must be migrated together will all its memory status. The total transfer size will be comparable with migrating a whole virtual machine(VM). So, it's still not feasible for edge computing environment. 

For Docker containers, there is a sample migration support in P.Haul's source code, which supports only an older version of docker-1.9.0-dev. And it's also extended by Ross Boucher to support docker-1.10-dev. However, as we will discussed in section \ref{migpractice}, we find this implementation will also transmit the entire file system for that container, regardless of the layered storage of Docker platform, which will make the migration vital slow in the edge of WAN network. 

Therefore, in this paper, we investigate the inner details of Docker storage in order to find out how to leverage the layered storage of Docker platform during the migration and avoid the transfer of unnecessary docker images. 

%  a lightweight universal runtime container. runC is a low-level container runtime and an implementation of the Open Container Initiative specification. runC exposes and expects a user to understand low-level details of the host operating system and configuration.

