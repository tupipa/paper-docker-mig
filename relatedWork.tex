In this section, we discuss related work in the areas of dynamic VM synthesis, VM handoff, and container migration.

\subsection{Dynamic VM Synthesis and VM Hand-off} \label{motivation:vmhandoff}

% Many researches have been done in order to accelerate the service handoff across edge servers.
Techniques based on virtual machine migration have been proposed in  \cite{ha2015vmhandoff} \cite{satya2009case} to accelerate the service handoff across edge servers. 
% As discussed in irtual Machine live migration in data centers could not be used for service handoff.
% which was well discussed in \cite{ha2015vmhandoff}. Firstly, Live migration of VM requires high bandwidth of connection and transfers Gigabytes of data in each migration, where service handoff for offloading happens on low bandwidth WANs. Secondly, live migration in data centers aims at short duration of the VM down time. However, service handoff across edge servers requires to optimize the time of total operations from the beginning of handoff request. 

In order to enable high speed service handoff based on virtual machine migration techniques, Satyanarayanan \textit{et al.} in \cite{satya2009case} proposed VM synthesis to divide huge VM images into a base VM image and a relatively small overlay image for one specific application. Before migration starts, the server is assumed to have a base image, so that the transition of this base image is avoided. During migration, only the application overlay image is transferred from the source to the target server.

Based on the work of VM synthesis, VM handoff across Cloudlet servers (alias of edge servers) was proposed in \cite{ha2015vmhandoff}. VM handoff allows the mobile client to seamlessly transfer the runtime status of its VM from one edge sever to a nearer edge server. Besides VM synthesis, it uses dirty tracking for both file system and main memory  to reduce the transferred size during handoff.
Additionally, it also uses compression and pipelined processing to reduce transfer time, and algorithms to adapt to different network conditions.
Finally, it highly reduced the transfer size and migration time under WAN environments.

While significant efforts have been invested into VM synthesis and handoff, the performance cannot meet the requirements of offloading services in edge computing environments, given the mobility of clients, dominance of wireless access, geographical distribution of edge nodes, and real-time interaction requirement. Instead,  our work is along the lines of container, a lightweight OS-level virtualization technique, with focus on the migration of containers to reduce the total handoff time. 
% However,
% % the transfer size of the application still had tens or hundreds of megabytes on average. 
% the total handoff time was still several minutes on a WAN network. For example, it will requires $245.5$ seconds to migrate a running OpenFace instance under 5Mbps bandwidth (50ms latency) network in \cite{ha2015vmhandoff}. 

% One of the reasons for the long latency of handoff the large image size we need to transfer. Although the VM synthesis could reduce the image size by splitting images into multiply layers and only transfer the application-specific layer, the total transferred size is still in the magnitude of tens of megabytes and even hundreds of megabytes. 
% The application layer was encapsulated with the whole application, both its static binary programs and its runtime memory data, which we think is an unnecessary cost. 

% On the other hand, the deployment of the VM synthesis system is challenging for the legacy system. In order to enable VM synthesis, the hypervisor of VMs need to be patched to track the dirty memory at runtime. Also the storage of VM images need to be adapted to Linux FUSE interfaces in order to track file system changes inside the running VMs. Those two changes are hard to deploy in practice since they changes the behavior of hypervisors and file systems of the legacy platform and will also cause lots of performance overhead. 

% \subsection{Latency Sensitive Mobile Applications}

% \subsubsection{Enlightened VM Post-Copy} In order to improve the 
% \subsection{Linux Containers}

\subsection{Containers Migration}
% Useful link: https://coreos.com/rkt/docs/latest/rkt-vs-other-projects.html#rkt-vs-runc

% Containers provide operating system level virtualization by running processes in isolated environments, 
Containers provide operating system level virtualization by running a group of processes in isolated environments. It is supported by the kernel features of namespace 
 and cgroups (control groups)\cite{rosen2014container}.
Namespaces are used to provide a private view of system resources for processes \cite{pike1992plan9}, while cgroups are used to restrict the quantity of resources a group of processes can access\cite{rosen2013namespaceIntro}.  

%  every resource in the system, either local or remote, is represented by a hierarchical file system; and a user or process assembles a private view of the system by constructing a file name space that connects these resources.

Container runtime is a tool that provides an easy-to-use API for managing containers by abstracting the low-level technical details of namespaces and cgroups. 
Although creating a container by crafting cgroup and namepaces step by step is possible \cite{conScratch}, managing containers by container runtime is much easier.  Such tools include LXC/LXD\cite{LXC} ,
runC\cite{runc},
rkt\cite{rkt}, 
OpenVZ\cite{openvz}, 
Docker\cite{docker}, etc.. Different container runtime has different scenerios of usage. For example, LXC/LXD only cares about full system containers and doesn't care about what kind of application runs inside the container, while Docker aims to encapsulate a specific application in the container.  

Migration of containers becomes possible when CRIU (Checkpoint/Restore In Userspace)\cite{criu} supports the checkpoint/restore functionality for Linux. 
Now CRIU supports the checkpoint and restore of containers for OpenVZ, LXC, and Docker. 


Based on CRIU, OpenVZ now supports migration of containers. The implementation can be found from the CRIU community's open source project, P.Haul ( process Hauler ) \cite{phaul}.  It is claimed that migration could be done within 5 seconds\cite{livmigcon}.
However, OpenVZ uses a distributed storage system~\cite{openvzfs}, where all files are shared across a high bandwidth network. 
This means that during container migration, it only needs to migrate the checkpointed memory images and no file transfer needs to be done. 
However, due to the limited WAN bandwidth for edge servers, it is not possible to deploy distributed storage. Therefore, migration of OpenVZ containers is not suitable for service handoff on edge computing platforms.

For LXC containers, the migration is implemented in Qiu's thesis work~\cite{qiu2016evaluating}, which is also based on the CRIU project. However, LXC regards containers as a whole system container, and there is no layered storage for containers. Therefore, during container migration, all of the file system for that container must be migrated together with all memory status. The total transfer size will be comparable with migrating a whole virtual machine(VM). So, this is still not feasible for an edge computing environment. 

For Docker containers, there is a sample migration helper in P.Haul's source code, which supports only an older version of docker-1.9.0-dev. It is also extended by Ross Boucher to support docker-1.10-dev. However, as we have discussed in section \ref{migpractice}, we find this implementation transmits the entire file system of that container, regardless of the layered storage of the Docker platform. This makes the migration unsatisfactorily slow across the edges of the WAN. Therefore, in this paper, we dive into the inner technical details of Docker storage and investigate leveraging the layered storage of Docker platform during migration. Finally we avoid the transfer of unnecessary file system data by sharing container image layers between different Docker hosts. 

%  a lightweight universal runtime container. runC is a low-level container runtime and an implementation of the Open Container Initiative specification. runC exposes and expects a user to understand low-level details of the host operating system and configuration.

