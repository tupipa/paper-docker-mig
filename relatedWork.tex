
\subsection{Emerging Applications in Mobile Edge Computing}


With the advancement of edge computing, many researchers have developed various kinds of applications to benefit from the high data rates and low latency of the edge computing platforms.
One example of such services is Augmented Reality (AR), where application on a mobile device overlays augmented reality content
onto objects viewed on the device's camera and edge servers can
provide local object tracking and local AR content caching
\cite{satya2009case,MEC2014initiative,MEC2015-5G}.
% The solution minimizes round trip time and maximizes throughput for optimum quality of experience. 
% It can be used to offer consumer or enterprise propositions, such as tourist information, sporting event information, advertisements etc.

% Sataya Gabriel 
Gabriel platform \cite{ha2014wearable}  was proposed with many wearable cognitive assistance applications using a Glass-like wearable device, such as Lego Assistant, Drawing Assistant, Ping-pong Assistant,  and Assistance from Crowd-sourced Videos. Those applications could react to a user's actions in real time with predefined guidance or guidance from crowd-sourced videos. 


% openface
OpenFace\cite{openface2016} is a real-time face recognition application for mobile scenarios that offers high accuracy with low training and prediction times. Mobile clients send the pictures captured from the camera to its near server. The server runs face recognition with deep neural networks and send the feedback to mobile clients in real time.  
More similar applications are also shown in \cite{satya2017edge}.

All the above applications require to offload heavy workloads, such as computer vision and signal processing, to the edge server and get response in real time. However, there are still lots of challenges before we can really deploy this edge cloud infrastructure. 

% \cite{satya2009case}, 
% \cite{MEC2014initiative}, 
% \cite{MEC2015-5G},  
% \cite{yi2015fog},
% \cite{yi2015survey},  
% \cite{ satya2017edge}.


% \subsection{Service Hand-off Across Edge Servers}

\subsection{Seamless Handoff Across 
% Cellular Networks vs. 
Edge Servers}

Under the environment of Mobile Edge Computing, most applications can benefit from offloading its heavy workloads to the edge servers and get nearly real time responsive services. However, this relies on the relative near distance to the edge servers under a WAN network. When the mobile clients moves far away from its current offloading edge server, the benefits of offloading will be reduced dramatically.

So one of the biggest challenges for mobile edge computing is to enable the free mobility for the offloading computations on the edge servers along with the mobile devices. 

In the centralized cloud infrastructure, mobility of clients are well guaranteed since the cloud server is always connected through the long latency WAN internet. However, in the edge computing infrastructure, mobile devices are connected to the nearby edge servers within limited network coverage scope. Therefore, when the mobile device moves far away from the edge server, it might be out of service, or alternatively, it has to handoff from the current edge server to a nearer edge server. 

This is very similar to the \textit{seamless hand-off } mechanism in the cellular networks, where the moving client can seamlessly hand-off across base stations and be served by its nearest available station. In cellular networks, changing a base station for a client is just to rebuild a wireless connection. Most of the run-time states of the services are still available either on the client or on the remote server. After re-connection, the run-time state can be quickly resumed seamlessly. 

However, there is one key difference between the cellular network hand-off and edge server hand-off. In the edge infrastructure, most mobile devices are using edge server to offload the resource intensive computing. This means the edge server will hold the states of all those resource intensive workloads. If we mobile hand-off from one edge server to another, just rebuilding the connection is certainly not enough. All the offloaded workloads are also need to be transferred from the current edge server to the next edge server.  

One possible solution is to use the live migration\cite{vmlivemig} to migrate a virtual machine from on edge server to another in order to seamlessly transfer the offloading workloads. However, this approach is already shown to be not suitable for edge computing environment in  \cite{ha2015vmhandoff}. This is because live migration is originally designed for high performance data centers with high bandwidth network. However, this is not possible for edge servers which are deployed on the edge of the WAN. Furthermore, live migration relies on shared storage so that it transfers only run-time state and doesn't transfer the disk storage data. Apparently, shared storage across the edge computing infrastructure is not feasible due to its widely distributed and low bandwidth feature. 

In order to enable hand-off across edge computing servers, many researches have been done based on virtual machine techniques. 

% \subsection{Fog/Edge Computing}
% \subsection{Virtual Machine Live Migration}
% \subsection{Cloudlet}

\subsection{Dynamic VM Synthesis and VM Hand-off} \label{motivation:vmhandoff}

% Many researches have been done in order to accelerate the service handoff across edge servers.
Techniques based on virtual machine migration have been proposed in  \cite{ha2015vmhandoff} \cite{satya2009case} to accelerate the service handoff across edge servers. Virtual Machine live migration in data centers could not be used for service handoff, which was well discussed in \cite{ha2015vmhandoff}. Firstly, Live migration of VM requires high bandwidth of connection and transfers Gigabytes of data in each migration, where service handoff for offloading happens on low bandwidth WANs. Secondly, live migration in data centers aims at short duration of the VM down time. However, service handoff across edge servers requires to optimize the time of total operations from the beginning of handoff request. 

In order to enable high speed service hand-off based on virtual machine migration techniques, Satyanarayanan \textit{et al.} in \cite{satya2009case} proposed VM synthesis to divide a huge VM imges into a base VM image and a relatively small overlay image for one specific application. Before migration starts, the server is assumed to have a base image, so that the transition of this base image is avoided. During migration, only the application's overlay image is transferred from the source to the target server.

Based on the work of VM synthesis, VM handoff across Couldlet servers (alias of edge servers) was proposed in \cite{ha2015vmhandoff}. VM handoff allows the mobile client to seamlessly transfer the runtime status of its VM from one edge sever to a nearer edge server. Besides VM synthesis, it uses dirty tracking for both file system and main memory  to reduce the transferred size during handoff.
Additionaly, it also uses compression, pipelined processing to reduce transfer time and algorithms to adapt to different network conditions.
Finally, it highly reduced the transferred size and migration time under WAN network environments.

However,
% the transfer size of the application still had tens or hundreds of megabytes on average. 
the total hand-off time was still several minutes on a WAN network. For example, it will requires $245.5$ seconds to migrate a running OpenFace instance under 5Mbps bandwidth (50ms latency) network in \cite{ha2015vmhandoff}. 

One of the reasons for the long latency of handoff the large image size we need to transfer. Although the VM synthesis could reduce the image size by splitting images into multiply layers and only transfer the application-specific layer, the total transferred size is still in the magnitude of tens of megabytes and even hundreds of megabytes. 
The application layer was encapsulated with the whole application, both its static binary programs and its runtime memory data, which we think is an unnecessary cost. 

On the other hand, the deployment of the VM synthesis system is challenging for the legacy system. In order to enable VM synthesis, the hypervisor of VMs need to be patched to track the dirty memory at runtime. Also the storage of VM images need to be adapted to Linux FUSE interfaces in order to track file system changes inside the running VMs. Those two changes are hard to deploy in practice since they changes the behavior of hypervisors and file systems of the legacy platform and will also cause lots of performance overhead. 

% \subsection{Latency Sensitive Mobile Applications}

% \subsubsection{Enlightened VM Post-Copy} In order to improve the 
% \subsection{Linux Containers}

\subsection{Containers and Live Migration}
% Useful link: https://coreos.com/rkt/docs/latest/rkt-vs-other-projects.html#rkt-vs-runc

% Containers provide operating system level virtualization by running processes in isolated environments, 
Containers provide operating system level virtualization by running a group of processes in isolated environments. It's supported by the kernel features of namespace 
 and cgroup (control groups)\cite{rosen2014container}.
Namespaces are used to provide a private view of system resources for processes \cite{pike1992plan9}, while cgroups are used to restrict how many resources a group of processes can be accessed\cite{rosen2013namespaceIntro}.  

%  every resource in the system, either local or remote, is represented by a hierarchical file system; and a user or process assembles a private view of the system by constructing a file name space that connects these resources.

Container runtime is a tool that provides easy-to-use API for managing containers by abstracting the low-level technical details of namespaces and cgroups. 
Although creating a container by crafting cgroup and namepaces step by step is possible \cite{conScratch}, managing containers by container runtime is much more easier.  Such tools include LXC/LXD\cite{LXC} ,
runC\cite{runc},
rkt\cite{rkt}, 
OpenVZ\cite{openvz}, 
Docker\cite{docker}, etc. Different container runtime has different scenerios of usage. For example, LXC/LXD only cares about full system containers and doesn’t care about what kind of application runs inside the container, while Docker aims to encapsulate a specific application in the container.  

Live migration of containers become possible when CRIU (Checkpoint/Restore In Userspace)\cite{criu} supports the checkpoint/restore functionality for Linux. 
Now CRIU supports the checkpoint and restore of containers for OpenVZ, LXC, and Docker. 


Based on CRIU, OpenVZ now supports live migration of containers. The implementation can be found from the CRIU community's open source project, P.Haul(process Hauler)\cite{phaul}.  It's claimed that the migration could be done within 5 seconds\cite{livmigcon}.
However, OpenVZ uses distributed storage system\cite{openvzfs}, where all files are shared across high bandwidth network. 
This means when it live migrates the containers, it only needs to migrate the checkpointed memory images and no file transfer need to be done. 
However, due to the limited WAN bandwidth for edge servers, it's not possible to deploy distributed storage. Therefore, live migration of OpenVZ containers is not suitable for service handoff on edge computing platforms.

For LXC containers, the live migration is implemented in Yuqing Qiu's thesis work \cite{qiu2016evaluating}, which is also based on the CRIU project. However, LXC regards container as a whole system container, and they don't have layered storage for containers. Therefore, when containers are migrated, all the file system for that container must be migrated together will all its memory status. The total transfer size will be comparable with migrating a whole virtual machine(VM). So, it's still not feasible for edge computing environment. 

For Docker containers, there is a sample migration support in P.Haul's source code, which supports only an older version of docker-1.9.0-dev. And it's also extended by Ross Boucher to support docker-1.10-dev. However, as we will discussed in section \ref{migpractice}, we find this implementation will also transmit the entire file system for that container, regardless of the layered storage of Docker platform, which will make the migration vital slow in the edge of WAN network. 

Therefore, in this paper, we investigate the inner details of Docker storage in order to find out how to leverage the layered storage of Docker platform during the migration and avoid the transfer of unnecessary docker images. 

%  a lightweight universal runtime container. runC is a low-level container runtime and an implementation of the Open Container Initiative specification. runC exposes and expects a user to understand low-level details of the host operating system and configuration.

